<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Flow-based language models with continuous denoising enable one-step parallel text generation.">
  <meta name="keywords" content="FLM, Language Models, Flow Matching, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FLM: Continuous Denoising Enables One-step Language Modeling</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <style>
        sheel-comment {
            display: inline-block; /* change to none for production */
            background-color: #fff3cd; /* Soft yellow */
            color: #856404;            /* Deep brown/gold */
            border-left: 4px solid #ffeeba;
            padding: 2px 8px;
            margin: 0 5px;
            font-size: 0.85rem;
            font-family: sans-serif;
            font-style: italic;
            border-radius: 4px;
            cursor: help;
        }

        /* Adds a "Review:" prefix automatically */
        sheel-comment::before {
            content: "Sheel: ";
            font-weight: bold;
            text-transform: uppercase;
            font-size: 0.7rem;
            margin-right: 4px;
        }
    </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/david3684/flm">
      <span class="icon">
          <i class="fab fa-github"></i>
      </span>
      </a>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Continuous Denoising Enables One-step Language Modeling</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Chanhyuk Lee<sup>1</sup>,</span>
            <span class="author-block">
              Jaehoon Yoo<sup>1</sup>,</span>
            <span class="author-block">
              Manan Agarwal<sup>2</sup>,
            </span>
            <span class="author-block">
              Sheel Shah<sup>2</sup>,
            </span>
            <span class="author-block">
              Jerry Huang<sup>2</sup>,
            </span>
            <span class="author-block">
              Aditi Raghunathan<sup>2</sup>,
            </span>
            <span class="author-block">
              Seunghoon Hong<sup>1</sup>,
            </span>
            <span class="author-block">
              Nicholas M. Boffi<sup>†2</sup>,
            </span>
            <span class="author-block">
              Jinwoo Kim<sup>†1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KAIST,</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
            <br>
            <span class="author-block"><sup>†</sup>Equal advising</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/david3684/flm"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- TL;DR -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-5">TL;DR</h2>
        <div class="content has-text-justified">
          <p>
            We introduce Flow-based Language Models (FLM), a method that enables <strong>one-step parallel text generation</strong>
            through continuous denoising, achieving an <strong>8.3× speedup</strong> over existing approaches. FLM outperforms discrete diffusion in both quality and speed.
          </p>
          <sheel-comment>
            Add 8.3x figure here
          </sheel-comment>
        </div>
      </div>
    </div>
    <!--/ TL;DR -->
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Language models based on discrete diffusion have attracted interest for their potential for faster generation than autoregressive models. 
            In practice, however, they are challenged by a sharp degradation of sample quality in the few-step regime.
          </p>
          <p>
            We show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. 
            By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising on one-hot token representations. 
            The model is trained to predict clean data via multi-token classification, leveraging a simple time reparameterization that greatly improves training and generation.
          </p>
          <p>
            By distilling FLM into its associated flow map, we obtain a distilled language model (FLM-Distill) capable of few-step generation. 
            On the LM1B and OWT language datasets, FLM attains generation quality competitive with state-of-the-art discrete diffusion models. With FLM-Distill, we outperform recent few-step language models across the board, with <i>one-step generation</i> already approaching their 8-step quality.
            Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Flow-based Language Model</h2>
        <div class="content has-text-justified">
          <p>
            FLM applies the benefits of continuous image generation in discrete state spaces by encoding
            text as one-hot vectors and using flow matching to directly map noise to one-hot data.
            FLM uses the same multi-token classification objective as discrete diffusion models, but instead of 
            updating the discrete state from one-token to another, FLM gradually denoises all tokens in parallel.
            This enables FLM to represent a superposition of sequences while also capturing correlations between tokens &#8212;
            a fundamental bottleneck with discrete diffusion models in the few-step regime. 
            <sheel-comment>Add overview gif here.</sheel-comment>
          </p>
          <p>
            We revisit the fundamentals of flows over discrete modalities and leverage a novel time-reparametrization
            to enable efficient training of FLM. Finally, we use Flow Map Distillation to distill our multi-step flow model into its associated flow map (FLM-distill), 
            enabling one-step generation with minimal loss in quality.
          </p>
          <p>
            <img src="figures/combined_diffusion_comparison.png" alt="Comparison between FLM and Discrete Diffusion" width="100%">
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">One-step Generated Samples</h2>
        <div class="content has-text-justified">
          <sheel-comment>Add one-step generated samples here. @Manan copy samples from figure 7 in paper and ask Claude to put it in a nice format (keep red color highlighting too)</sheel-comment>
        </div>
      </div>
    </div>
  </div>
</section>
          

<sheel-comment>Ignore stuff below this point</sheel-comment>
<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            FLM builds upon recent advances in flow matching, diffusion models, and non-autoregressive generation.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2210.02747">Flow Matching</a> introduces the continuous normalizing flow framework that enables efficient training of generative models.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2107.03006">Diffusion-LM</a> and <a href="https://arxiv.org/abs/2205.14217">Continuous Diffusion</a> explore diffusion models for text generation, providing foundations for continuous denoising in language modeling.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2202.00512">Self-Conditioned Embedding Diffusion</a> and <a href="https://arxiv.org/abs/2211.17106">GENIE</a> investigate non-autoregressive generation methods for language.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{lee2025flm,
  author    = {Lee, Chanhyuk and Yoo, Jaehoon and Agarwal, Manan and Shah, Sheel and Huang, Jerry and Raghunathan, Aditi and Hong, Seunghoon and Boffi, Nicholas M. and Kim, Jinwoo},
  title     = {Continuous Denoising Enables One-step Language Modeling},
  journal   = {arXiv preprint},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/david3684/flm">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
